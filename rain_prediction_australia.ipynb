{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# オーストラリア天気予測 - 明日の雨予測モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. ライブラリのインポートと設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 22\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     accuracy_score, precision_score, recall_score, f1_score,\n\u001b[0;32m     18\u001b[0m     roc_auc_score, average_precision_score, confusion_matrix,\n\u001b[0;32m     19\u001b[0m     roc_curve, precision_recall_curve\n\u001b[0;32m     20\u001b[0m )\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpruners\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MedianPruner\n\u001b[0;32m     25\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix,\n",
    "    roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "DATA_PATH = Path('data/weatherAUS.csv')\n",
    "FIGURES_PATH = Path('figures')\n",
    "EXPERIMENTS_PATH = Path('experiments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. データ読み込みと探索的データ分析 (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f'データサイズ: {df.shape}')\n",
    "print(f'期間: {df[\"Date\"].min()} ~ {df[\"Date\"].max()}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損値ヒートマップ\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "missing_matrix = df.isnull().astype(int)\n",
    "sns.heatmap(missing_matrix.T, cbar=True, yticklabels=df.columns, cmap='YlOrRd', ax=ax)\n",
    "ax.set_title('Missing Values Heatmap', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_PATH / 'missing_values.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ターゲット分布\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "target_counts = df['RainTomorrow'].value_counts()\n",
    "colors = ['#3498db', '#e74c3c']\n",
    "axes[0].bar(target_counts.index, target_counts.values, color=colors)\n",
    "axes[0].set_title('RainTomorrow Distribution (Count)')\n",
    "for i, v in enumerate(target_counts.values):\n",
    "    axes[0].text(i, v + 1000, f'{v:,}', ha='center')\n",
    "target_pct = df['RainTomorrow'].value_counts(normalize=True) * 100\n",
    "axes[1].pie(target_pct.values, labels=target_pct.index, autopct='%1.1f%%', colors=colors)\n",
    "axes[1].set_title('RainTomorrow Distribution (%)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_PATH / 'target_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'クラス不均衡比率: No:Yes = {target_counts[\"No\"]/target_counts[\"Yes\"]:.2f}:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相関行列\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r', center=0, ax=ax, annot_kws={'size': 8})\n",
    "ax.set_title('Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_PATH / 'correlation_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. データ前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ターゲット欠損削除と時系列分割\n",
    "df_clean = df.dropna(subset=['RainTomorrow']).copy()\n",
    "df_clean['Date'] = pd.to_datetime(df_clean['Date'])\n",
    "df_clean = df_clean.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "train_end = '2015-06-30'\n",
    "val_end = '2016-06-30'\n",
    "train_mask = df_clean['Date'] <= train_end\n",
    "val_mask = (df_clean['Date'] > train_end) & (df_clean['Date'] <= val_end)\n",
    "test_mask = df_clean['Date'] > val_end\n",
    "\n",
    "print(f'Train: {train_mask.sum():,} | Val: {val_mask.sum():,} | Test: {test_mask.sum():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理\n",
    "cat_cols = ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday']\n",
    "num_cols = [c for c in df_clean.columns if c not in cat_cols + ['Date', 'RainTomorrow']]\n",
    "\n",
    "df_imputed = df_clean.copy()\n",
    "train_data = df_imputed[train_mask]\n",
    "for col in num_cols:\n",
    "    df_imputed[col] = df_imputed[col].fillna(train_data[col].median())\n",
    "for col in cat_cols:\n",
    "    mode_val = train_data[col].mode().iloc[0] if len(train_data[col].mode()) > 0 else 'Unknown'\n",
    "    df_imputed[col] = df_imputed[col].fillna(mode_val)\n",
    "\n",
    "# 風向サイクリカルエンコーディング\n",
    "wind_directions = ['N', 'NNE', 'NE', 'ENE', 'E', 'ESE', 'SE', 'SSE', 'S', 'SSW', 'SW', 'WSW', 'W', 'WNW', 'NW', 'NNW']\n",
    "wind_to_angle = {d: i * (360 / 16) for i, d in enumerate(wind_directions)}\n",
    "for col in ['WindGustDir', 'WindDir9am', 'WindDir3pm']:\n",
    "    angles = df_imputed[col].map(wind_to_angle).fillna(0)\n",
    "    df_imputed[f'{col}_sin'] = np.sin(np.deg2rad(angles))\n",
    "    df_imputed[f'{col}_cos'] = np.cos(np.deg2rad(angles))\n",
    "\n",
    "df_imputed['RainToday'] = (df_imputed['RainToday'] == 'Yes').astype(int)\n",
    "df_imputed['RainTomorrow'] = (df_imputed['RainTomorrow'] == 'Yes').astype(int)\n",
    "le = LabelEncoder()\n",
    "df_imputed['Location_encoded'] = le.fit_transform(df_imputed['Location'])\n",
    "\n",
    "feature_cols = num_cols + ['RainToday', 'Location_encoded'] + \\\n",
    "    [f'{c}_sin' for c in ['WindGustDir', 'WindDir9am', 'WindDir3pm']] + \\\n",
    "    [f'{c}_cos' for c in ['WindGustDir', 'WindDir9am', 'WindDir3pm']]\n",
    "\n",
    "print(f'特徴量数: {len(feature_cols)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ分割と標準化\n",
    "X_train = df_imputed.loc[train_mask, feature_cols].values\n",
    "y_train = df_imputed.loc[train_mask, 'RainTomorrow'].values\n",
    "X_val = df_imputed.loc[val_mask, feature_cols].values\n",
    "y_val = df_imputed.loc[val_mask, 'RainTomorrow'].values\n",
    "X_test = df_imputed.loc[test_mask, feature_cols].values\n",
    "y_test = df_imputed.loc[test_mask, 'RainTomorrow'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_t = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_t = torch.FloatTensor(y_train).to(device)\n",
    "X_val_t = torch.FloatTensor(X_val_scaled).to(device)\n",
    "y_val_t = torch.FloatTensor(y_val).to(device)\n",
    "X_test_t = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_t = torch.FloatTensor(y_test).to(device)\n",
    "\n",
    "pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f'正例の重み: {pos_weight:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. モデル定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, dropout=0.3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.extend([nn.Linear(prev, h), nn.BatchNorm1d(h), nn.ReLU(), nn.Dropout(dropout)])\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def evaluate(y_true, y_proba):\n",
    "    y_pred = (y_proba >= 0.5).astype(int)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'auc_roc': roc_auc_score(y_true, y_proba),\n",
    "        'auc_pr': average_precision_score(y_true, y_proba)\n",
    "    }\n",
    "\n",
    "print('モデル定義完了')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. ロジスティック回帰のチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lr(trial):\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-1, log=True)\n",
    "    wd = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n",
    "    bs = trial.suggest_categorical('batch_size', [128, 256, 512])\n",
    "\n",
    "    model = LogisticRegressionModel(X_train_t.shape[1]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]).to(device))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    loader = DataLoader(TensorDataset(X_train_t, y_train_t.unsqueeze(1)), batch_size=bs, shuffle=True)\n",
    "\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        for bx, by in loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(bx), by)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        proba = torch.sigmoid(model(X_val_t)).cpu().numpy().flatten()\n",
    "    return roc_auc_score(y_val, proba)\n",
    "\n",
    "print('=== ロジスティック回帰のハイパーパラメータチューニング ===')\n",
    "study_lr = optuna.create_study(direction='maximize')\n",
    "study_lr.optimize(train_lr, n_trials=10, show_progress_bar=True)\n",
    "print(f'Best Val AUC-ROC: {study_lr.best_value:.4f}')\n",
    "print(f'Best Params: {study_lr.best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最良パラメータで再学習\n",
    "bp = study_lr.best_params\n",
    "model_lr = LogisticRegressionModel(X_train_t.shape[1]).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]).to(device))\n",
    "optimizer = optim.Adam(model_lr.parameters(), lr=bp['lr'], weight_decay=bp['weight_decay'])\n",
    "loader = DataLoader(TensorDataset(X_train_t, y_train_t.unsqueeze(1)), batch_size=bp['batch_size'], shuffle=True)\n",
    "\n",
    "lr_history = {'train_loss': [], 'val_loss': [], 'val_auc': []}\n",
    "for epoch in range(100):\n",
    "    model_lr.train()\n",
    "    for bx, by in loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model_lr(bx), by)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model_lr.eval()\n",
    "    with torch.no_grad():\n",
    "        tl = criterion(model_lr(X_train_t), y_train_t.unsqueeze(1)).item()\n",
    "        vl = criterion(model_lr(X_val_t), y_val_t.unsqueeze(1)).item()\n",
    "        vp = torch.sigmoid(model_lr(X_val_t)).cpu().numpy().flatten()\n",
    "        va = roc_auc_score(y_val, vp)\n",
    "    lr_history['train_loss'].append(tl)\n",
    "    lr_history['val_loss'].append(vl)\n",
    "    lr_history['val_auc'].append(va)\n",
    "\n",
    "# 学習曲線\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].plot(lr_history['train_loss'], label='Train')\n",
    "axes[0].plot(lr_history['val_loss'], label='Val')\n",
    "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss'); axes[0].legend()\n",
    "axes[0].set_title('Logistic Regression - Loss')\n",
    "axes[1].plot(lr_history['val_auc'], color='green')\n",
    "axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('AUC-ROC')\n",
    "axes[1].set_title('Logistic Regression - AUC')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_PATH / 'learning_curves/logistic_regression.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "model_lr.eval()\n",
    "with torch.no_grad():\n",
    "    y_proba_lr = torch.sigmoid(model_lr(X_test_t)).cpu().numpy().flatten()\n",
    "metrics_lr = evaluate(y_test, y_proba_lr)\n",
    "print(f'Test AUC-ROC: {metrics_lr[\"auc_roc\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. MLPのチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(trial):\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    hidden = [trial.suggest_int(f'h{i}', 64, 256, step=64) for i in range(n_layers)]\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.4)\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "    bs = trial.suggest_categorical('batch_size', [128, 256])\n",
    "\n",
    "    model = MLPModel(X_train_t.shape[1], hidden, dropout).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]).to(device))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loader = DataLoader(TensorDataset(X_train_t, y_train_t.unsqueeze(1)), batch_size=bs, shuffle=True)\n",
    "\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        for bx, by in loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(bx), by)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        proba = torch.sigmoid(model(X_val_t)).cpu().numpy().flatten()\n",
    "    return roc_auc_score(y_val, proba)\n",
    "\n",
    "print('=== MLPのハイパーパラメータチューニング ===')\n",
    "study_mlp = optuna.create_study(direction='maximize')\n",
    "study_mlp.optimize(train_mlp, n_trials=10, show_progress_bar=True)\n",
    "print(f'Best Val AUC-ROC: {study_mlp.best_value:.4f}')\n",
    "print(f'Best Params: {study_mlp.best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最良パラメータで再学習\n",
    "bp = study_mlp.best_params\n",
    "hidden = [bp[f'h{i}'] for i in range(bp['n_layers'])]\n",
    "model_mlp = MLPModel(X_train_t.shape[1], hidden, bp['dropout']).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]).to(device))\n",
    "optimizer = optim.Adam(model_mlp.parameters(), lr=bp['lr'])\n",
    "loader = DataLoader(TensorDataset(X_train_t, y_train_t.unsqueeze(1)), batch_size=bp['batch_size'], shuffle=True)\n",
    "\n",
    "mlp_history = {'train_loss': [], 'val_loss': [], 'val_auc': []}\n",
    "best_auc = 0\n",
    "best_state = None\n",
    "for epoch in range(100):\n",
    "    model_mlp.train()\n",
    "    for bx, by in loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model_mlp(bx), by)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model_mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        tl = criterion(model_mlp(X_train_t), y_train_t.unsqueeze(1)).item()\n",
    "        vl = criterion(model_mlp(X_val_t), y_val_t.unsqueeze(1)).item()\n",
    "        vp = torch.sigmoid(model_mlp(X_val_t)).cpu().numpy().flatten()\n",
    "        va = roc_auc_score(y_val, vp)\n",
    "    mlp_history['train_loss'].append(tl)\n",
    "    mlp_history['val_loss'].append(vl)\n",
    "    mlp_history['val_auc'].append(va)\n",
    "    if va > best_auc:\n",
    "        best_auc = va\n",
    "        best_state = {k: v.clone() for k, v in model_mlp.state_dict().items()}\n",
    "\n",
    "model_mlp.load_state_dict(best_state)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].plot(mlp_history['train_loss'], label='Train')\n",
    "axes[0].plot(mlp_history['val_loss'], label='Val')\n",
    "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss'); axes[0].legend()\n",
    "axes[0].set_title('MLP - Loss')\n",
    "axes[1].plot(mlp_history['val_auc'], color='green')\n",
    "axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('AUC-ROC')\n",
    "axes[1].set_title('MLP - AUC')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_PATH / 'learning_curves/mlp.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "model_mlp.eval()\n",
    "with torch.no_grad():\n",
    "    y_proba_mlp = torch.sigmoid(model_mlp(X_test_t)).cpu().numpy().flatten()\n",
    "metrics_mlp = evaluate(y_test, y_proba_mlp)\n",
    "print(f'Test AUC-ROC: {metrics_mlp[\"auc_roc\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Random Forestのチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf(trial):\n",
    "    n_est = trial.suggest_int('n_estimators', 100, 300, step=50)\n",
    "    depth = trial.suggest_int('max_depth', 10, 30)\n",
    "    model = RandomForestClassifier(n_estimators=n_est, max_depth=depth, class_weight='balanced', n_jobs=-1, random_state=SEED)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    return roc_auc_score(y_val, model.predict_proba(X_val_scaled)[:, 1])\n",
    "\n",
    "print('=== Random Forestのハイパーパラメータチューニング ===')\n",
    "study_rf = optuna.create_study(direction='maximize')\n",
    "study_rf.optimize(train_rf, n_trials=10, show_progress_bar=True)\n",
    "print(f'Best Val AUC-ROC: {study_rf.best_value:.4f}')\n",
    "print(f'Best Params: {study_rf.best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最良パラメータで再学習\n",
    "bp = study_rf.best_params\n",
    "model_rf = RandomForestClassifier(n_estimators=bp['n_estimators'], max_depth=bp['max_depth'], class_weight='balanced', n_jobs=-1, random_state=SEED)\n",
    "model_rf.fit(X_train_scaled, y_train)\n",
    "y_proba_rf = model_rf.predict_proba(X_test_scaled)[:, 1]\n",
    "metrics_rf = evaluate(y_test, y_proba_rf)\n",
    "print(f'Test AUC-ROC: {metrics_rf[\"auc_roc\"]:.4f}')\n",
    "\n",
    "# 特徴量重要度\n",
    "feat_imp = pd.DataFrame({'feature': feature_cols, 'importance': model_rf.feature_importances_}).sort_values('importance', ascending=False)\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.barplot(data=feat_imp.head(15), x='importance', y='feature', ax=ax, palette='viridis')\n",
    "ax.set_title('Random Forest - Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_PATH / 'feature_importance/random_forest.png', dpi=150)\n",
    "plt.show()\n",
    "feat_imp.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. モデル比較と可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果まとめ\n",
    "results = {'Logistic Regression': metrics_lr, 'MLP': metrics_mlp, 'Random Forest': metrics_rf}\n",
    "results_df = pd.DataFrame(results).T.round(4)\n",
    "print('=== 全モデルの評価結果（テストデータ） ===')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価指標の比較棒グラフ\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "metrics_list = ['accuracy', 'precision', 'recall', 'f1', 'auc_roc', 'auc_pr']\n",
    "titles = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC', 'AUC-PR']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "for idx, (m, t) in enumerate(zip(metrics_list, titles)):\n",
    "    ax = axes[idx//3, idx%3]\n",
    "    vals = [results[k][m] for k in results]\n",
    "    bars = ax.bar(results.keys(), vals, color=colors)\n",
    "    ax.set_title(t); ax.set_ylim(0, 1)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    for bar, v in zip(bars, vals):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f'{v:.3f}', ha='center')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_PATH / 'model_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROCカーブ\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "for name, proba in [('Logistic Regression', y_proba_lr), ('MLP', y_proba_mlp), ('Random Forest', y_proba_rf)]:\n",
    "    fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "    ax.plot(fpr, tpr, label=f'{name} (AUC={roc_auc_score(y_test, proba):.4f})', linewidth=2)\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "ax.set_xlabel('FPR'); ax.set_ylabel('TPR'); ax.set_title('ROC Curve Comparison')\n",
    "ax.legend(); ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_PATH / 'roc_curves/comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混同行列\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for idx, (name, proba) in enumerate([('Logistic Regression', y_proba_lr), ('MLP', y_proba_mlp), ('Random Forest', y_proba_rf)]):\n",
    "    cm = confusion_matrix(y_test, (proba >= 0.5).astype(int))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx], xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "    axes[idx].set_title(name); axes[idx].set_xlabel('Predicted'); axes[idx].set_ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_PATH / 'confusion_matrices/comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. 結果の保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# チューニング結果をJSONに保存\n",
    "tuning_results = {\n",
    "    'logistic_regression': {'best_params': study_lr.best_params, 'best_val_auc': study_lr.best_value, 'test_metrics': {k: float(v) for k, v in metrics_lr.items()}},\n",
    "    'mlp': {'best_params': study_mlp.best_params, 'hidden_dims': hidden, 'best_val_auc': study_mlp.best_value, 'test_metrics': {k: float(v) for k, v in metrics_mlp.items()}},\n",
    "    'random_forest': {'best_params': study_rf.best_params, 'best_val_auc': study_rf.best_value, 'test_metrics': {k: float(v) for k, v in metrics_rf.items()}}\n",
    "}\n",
    "with open(EXPERIMENTS_PATH / 'tuning_results.json', 'w') as f:\n",
    "    json.dump(tuning_results, f, indent=2)\n",
    "print('チューニング結果を experiments/tuning_results.json に保存しました')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終結果サマリー\n",
    "print('\\n' + '='*60)\n",
    "print('           最終結果サマリー（テストデータ）')\n",
    "print('='*60)\n",
    "print('\\n【評価指標比較】')\n",
    "print(results_df.to_string())\n",
    "print('\\n【ベストモデル（AUC-ROC基準）】')\n",
    "best_model = results_df['auc_roc'].idxmax()\n",
    "print(f'  {best_model}: AUC-ROC = {results_df.loc[best_model, \"auc_roc\"]:.4f}')\n",
    "print('\\n' + '='*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CENT000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
